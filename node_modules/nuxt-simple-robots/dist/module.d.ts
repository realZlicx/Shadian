import * as _nuxt_schema from '@nuxt/schema';

type Arrayable<T> = T | T[];
interface RobotsGroupInput {
    comment?: Arrayable<string>;
    disallow?: Arrayable<string>;
    allow?: Arrayable<string>;
    userAgent?: Arrayable<string>;
}

interface ModuleOptions {
    /**
     * Whether the robots.txt should be generated.
     *
     * @default true
     */
    enabled: boolean;
    /**
     * Path to the sitemaps, if it exists.
     * Will automatically be resolved as an absolute path.
     *
     * @default []
     */
    sitemap: Arrayable<string>;
    /**
     * Paths to add to the robots.txt with the allow keyword.
     *
     * @default []
     */
    allow: Arrayable<string>;
    /**
     * Paths to add to the robots.txt with the disallow keyword.
     *
     * @default []
     */
    disallow: Arrayable<string>;
    /**
     * Define more granular rules for the robots.txt. Each stack is a set of rules for specific user agent(s).
     *
     * @default []
     * @example [
     *    {
     *    userAgents: ['AdsBot-Google-Mobile', 'AdsBot-Google-Mobile-Apps'],
     *    disallow: ['/admin'],
     *    allow: ['/admin/login'],
     *    },
     *  ]
     */
    groups: RobotsGroupInput[];
    /**
     * The value to use when the site is indexable.
     *
     * @default 'index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1'
     */
    robotsEnabledValue: string;
    /**
     * The value to use when the site is not indexable.
     *
     * @default 'noindex, nofollow'
     */
    robotsDisabledValue: string;
    /**
     * Should route rules which disallow indexing be added to the `/robots.txt` file.
     *
     * @default true
     */
    disallowNonIndexableRoutes: boolean;
    /**
     * Specify a robots.txt path to merge the config from, relative to the root directory.
     *
     * When set to `true`, the default path of `<publicDir>/robots.txt` will be used.
     *
     * When set to `false`, no merging will occur.
     *
     * @default true
     */
    mergeWithRobotsTxtPath: boolean | string;
    /**
     * Blocks bots that don't benefit our SEO and are known to cause issues.
     *
     * @default false
     */
    blockNonSeoBots: boolean;
    /**
     * Enables debug logs and a debug endpoint.
     *
     * @default false
     */
    debug: boolean;
    /**
     * Should the robots.txt display credits for the module.
     *
     * @default true
     */
    credits: boolean;
    /**
     * The url of your site.
     * Used to generate absolute URLs for the sitemap.
     *
     * Note: This is only required when prerendering your site.
     *
     * @deprecated Provide `url` through site config instead: `{ site: { url: <value> }}`.
     * This is powered by the `nuxt-site-config` module.
     * @see https://github.com/harlan-zw/nuxt-site-config
     */
    host?: string;
    /**
     * The url of your site.
     * Used to generate absolute URLs for the sitemap.
     *
     * Note: This is only required when prerendering your site.
     *
     * @deprecated Provide `url` through site config instead: `{ site: { url: <value> }}`.
     * This is powered by the `nuxt-site-config` module.
     * @see https://github.com/harlan-zw/nuxt-site-config
     */
    siteUrl?: string;
    /**
     * Can your site be crawled by search engines.
     *
     * @deprecated Provide `indexable` through site config instead: `{ site: { indexable: <value> }}`.
     * This is powered by the `nuxt-site-config` module.
     * @see https://github.com/harlan-zw/nuxt-site-config
     */
    indexable?: boolean;
}
interface ResolvedModuleOptions extends ModuleOptions {
    sitemap: string[];
    disallow: string[];
}
interface ModuleHooks {
    'robots:config': (config: ResolvedModuleOptions) => Promise<void> | void;
}
interface ModulePublicRuntimeConfig {
    ['nuxt-simple-robots']: ResolvedModuleOptions;
}
declare const _default: _nuxt_schema.NuxtModule<ModuleOptions>;

export { ModuleHooks, ModuleOptions, ModulePublicRuntimeConfig, ResolvedModuleOptions, _default as default };
